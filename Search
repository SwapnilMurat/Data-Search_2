import time
import csv
import random
import zipfile
import os

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import InvalidSessionIdException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager


LINKEDIN_EMAIL = "Email"
LINKEDIN_PASSWORD = "password"
INPUT_CSV = "company_city_list.csv"   
OUTPUT_CSV = "real_estate_professionals_india.csv"
OUTPUT_ZIP = "real_estate_professionals_india.zip"

NUM_QUERIES = 30                     
MAX_RESULTS_PER_QUERY = 5            
BATCH_SIZE = 5                       

companies = [
    "DLF Limited",
    "Oberoi Realty",
    "Lodha Group",
    "Prestige Estates Projects",
    "Godrej Properties",
    "Brigade Group",
    "Piramal Realty",
    "Sobha Ltd",
    "Macrotech Developers",
    "Kolte-Patil Developers"
]

cities = [
    "Mumbai", "Delhi", "Bengaluru", "Hyderabad", "Pune",
    "Chennai", "Ahmedabad", "Kolkata", "Noida", "Gurgaon"
]

# Real estate job titles we want to search
job_titles = [
    "Real Estate Agent",
    "Real Estate Consultant",
    "Property Advisor",
    "Property Manager",
    "Broker",
    "Realty Specialist"
]


driver = None

def init_driver():
    global driver
    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-blink-features=AutomationControlled")  # avoid detection
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def linkedin_login():
    driver.get("https://www.linkedin.com/login")
    time.sleep(3)

    driver.find_element(By.ID, "username").send_keys(LINKEDIN_EMAIL)
    driver.find_element(By.ID, "password").send_keys(LINKEDIN_PASSWORD)
    driver.find_element(By.ID, "password").send_keys(Keys.RETURN)
    time.sleep(5)
    print("[INFO] Logged into LinkedIn successfully!")

def restart_driver():
    global driver
    try:
        driver.quit()
    except:
        pass
    print("[INFO] Restarting ChromeDriver...")
    init_driver()
    linkedin_login()

def safe_get(url):
    global driver
    try:
        driver.get(url)
    except InvalidSessionIdException:
        print("[WARN] Session expired! Restarting driver...")
        restart_driver()
        driver.get(url)
    except WebDriverException as e:
        print(f"[WARN] WebDriverException: {e}. Restarting driver...")
        restart_driver()
        driver.get(url)


def generate_real_estate_queries():
    queries = []
    for _ in range(NUM_QUERIES):
        queries.append({
            "company": random.choice(companies),
            "city": random.choice(cities),
            "job_title": random.choice(job_titles)
        })

    with open(INPUT_CSV, "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["company", "city", "job_title"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(queries)

    print(f"[INFO] Generated {INPUT_CSV} with {NUM_QUERIES} targeted real estate queries")


def search_linkedin_real_estate(query, max_results=5):
    geo_urn_india = "%5B%22102105083%22%5D"
    search_url = f"https://www.linkedin.com/search/results/people/?keywords={query.replace(' ', '%20')}&geoUrn={geo_urn_india}&origin=GLOBAL_SEARCH_HEADER"

    print(f"[INFO] Visiting India-specific search URL: {search_url}")
    safe_get(search_url)
    time.sleep(random.uniform(6, 9))  

    profile_data = []
    try:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(3, 5))

        results = driver.find_elements(By.CSS_SELECTOR, "li.reusable-search__result-container")

        for res in results[:max_results]:
            try:
                profile_link = res.find_element(By.CSS_SELECTOR, "a.app-aware-link").get_attribute("href")
                name_element = res.find_element(By.CSS_SELECTOR, "span.entity-result__title-text span[aria-hidden='true']")
                name_text = name_element.text.strip()

                try:
                    job_company_element = res.find_element(By.CSS_SELECTOR, "div.entity-result__primary-subtitle")
                    job_company_text = job_company_element.text.strip()
                except:
                    job_company_text = "N/A"

                try:
                    location_element = res.find_element(By.CSS_SELECTOR, "div.entity-result__secondary-subtitle")
                    location_text = location_element.text.strip()
                except:
                    location_text = "N/A"

                if "linkedin.com/in/" in profile_link:
                    profile_data.append({
                        "name": name_text,
                        "linkedin_url": profile_link,
                        "current_position": job_company_text,
                        "location": location_text
                    })
            except Exception:
                continue

    except Exception as e:
        print(f"[WARN] No results found for: {query}, Error: {e}")

    return profile_data

def create_zip_file(csv_file, zip_file):
    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
        zipf.write(csv_file)
    print(f"[INFO] Created ZIP file: {zip_file}")

def main():
    generate_real_estate_queries()

    init_driver()
    linkedin_login()

    all_results = []

    with open(INPUT_CSV, "r", encoding="utf-8") as infile:
        reader = csv.DictReader(infile)
        queries = list(reader)

    for i in range(0, len(queries), BATCH_SIZE):
        batch = queries[i:i+BATCH_SIZE]
        print(f"\n[INFO] Processing batch {i//BATCH_SIZE + 1}/{(len(queries)+BATCH_SIZE-1)//BATCH_SIZE}")

        for row in batch:
            query = f"{row['job_title']} at {row['company']} in {row['city']} India"
            print(f"[INFO] Searching for: {query} (India-specific)")

            try:
                profiles = search_linkedin_real_estate(query, MAX_RESULTS_PER_QUERY)
            except InvalidSessionIdException:
                print("[WARN] Session lost mid-search, restarting...")
                restart_driver()
                profiles = search_linkedin_real_estate(query, MAX_RESULTS_PER_QUERY)

            if profiles:
                for p in profiles:
                    all_results.append({
                        "company": row['company'],
                        "city": row['city'],
                        "job_title": row['job_title'],
                        "name": p["name"],
                        "linkedin_url": p["linkedin_url"],
                        "current_position": p["current_position"],
                        "location": p["location"]
                    })
            else:
                all_results.append({
                    "company": row['company'],
                    "city": row['city'],
                    "job_title": row['job_title'],
                    "name": "NOT FOUND",
                    "linkedin_url": "NOT FOUND",
                    "current_position": "NOT FOUND",
                    "location": "NOT FOUND"
                })

            time.sleep(random.uniform(8, 15))

        restart_driver()

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as outfile:
        fieldnames = [
            "company",
            "city",
            "job_title",
            "name",
            "linkedin_url",
            "current_position",
            "location"
        ]
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_results)

    print(f"[DONE] Saved real estate professional data to {OUTPUT_CSV}")

    create_zip_file(OUTPUT_CSV, OUTPUT_ZIP)

    # Quit driver
    driver.quit()

    print("\n✅ DONE! Your India-specific data is ready.")
    print(f"➡ CSV File: {os.path.abspath(OUTPUT_CSV)}")
    print(f"➡ Downloadable ZIP: {os.path.abspath(OUTPUT_ZIP)}")


if __name__ == "__main__":
    main()
